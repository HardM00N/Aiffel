{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exp4_Lyricist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "`나는 밥을 []`에서 빈 칸에 들어갈 말이 `먹는다`라는 것을 큰 고민 없이 알 수 있다. `밥`은 통계적으로 `먹`히기 때문이다. `알바생이 커피를 []`라면 아마도 `만든다`가 정답일 것이다.  \n",
        "\n",
        "인공지능이 글을 이해하게 하는 방식도 위와 같다. 어떤 문법적인 원리를 통해서가 아니고, **수많은 글을 읽게 함으로써** `나는`, `밥을`, 그 다음이 `먹는다`라는 사실을 알게 하는 것이다. 그런 이유에서 **많은 데이터가 곧 좋은 결과**를 만들어낸다.  \n",
        "\n",
        "이 방식을 가장 잘 처리하는 인공지능 중 하나가 **순환신경망(RNN)**이다.  \n",
        "앞에서 `먹었다`를 만드는 법은 배웠지만, 가장 첫 시작인 `나는`은 어떻게 만들어야 할까?  \n",
        "\n",
        "이는 `<start>`라는 특수한 토큰을 맨 앞에 추가해 줌으로써 해결할 수 있다. `<start>`를 입력으로 받은 순환신경망은 다음 단어로 `나는`을 생성하고, **생성한 단어를 다시 입력으로 사용**한다. 이 순환적인 특성을 살려 순환신경망이라고 이름을 붙인 것이다.  \n",
        "\n",
        "그렇게 순차적으로 `밥을 먹었다.`까지 생성하고 나면, 인공지능은 다 만들었다는 사인으로 `<end>`라는 특수한 토큰을 생성한다. 즉, `<start>`가 문장의 시작에 더해진 입력 데이터(문제지)와, `<end>`가 문장의 끝에 더해진 출력 데이터(답안지)가 필요하며, 이는 **문장 데이터만 있으면 만들어 낼 수 있다**는 것 또한 알 수 있다."
      ],
      "metadata": {
        "id": "Fp5qf6HP6-le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \" 나는 밥을 먹었다 \"\n",
        "\n",
        "source_sentence = \"<start>\" + sentence\n",
        "target_sentence = sentence + \"<end>\"\n",
        "\n",
        "print(\"Source 문장:\", source_sentence)\n",
        "print(\"Target 문장:\", target_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5_jYLlP_t0p",
        "outputId": "228b238e-16f1-465f-d6e8-61ccdde4e72d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source 문장: <start> 나는 밥을 먹었다 \n",
            "Target 문장:  나는 밥을 먹었다 <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 언어 모델(Language Model)\n",
        "`나는`, `밥을`, `먹었다`를 순차적으로 생성할 때, `밥을` 다음이 `먹었다`인 것은 쉽게 알 수 있다. 하지만 `나는` 다음이 `밥을`인 것은 조금 억지처럼 느껴질 수 있다. 실제로 동작하는 방식도, `밥을`을 만드는 것은 순전히 운이다. 의도한다고 해서 나오는 것이 아니다.  \n",
        "\n",
        "이걸 조금 더 확률적으로 표현해보자.  \n",
        "\n",
        "'나는 밥을' 다음에 '먹었다'가 나올 확률을 p(먹었다 |  나는, 밥을)이라고 하자. 그렇다면 이 확률은 p(밥을 | 나는)보다는 높게 나올 것이다. 아마 p(먹었다 | 나는, 밥을, 맛있게)의 확률값은 더 높을 것이다.  \n",
        "\n",
        "어떤 문구 뒤에 다음 단어가 나올 확률이 높다는 것은 그 단어가 나오는 것이 보다 자연스럽다는 뜻이 된다. 그렇다면 '나는' 뒤에 '밥을'이 나오는 것이 자연스럽지 않다는 것일까? 그것은 아니다. '나는' 뒤에 올 수 있는 자연스러운 단어의 경우의 수가 워낙 많기에 불확실성이 높을 뿐이다.  \n",
        "\n",
        "n-1개의 단어 시퀀스 w1....wn-1가 주어졌을 때, n번째 단어 wn으로 무엇이 올지를 에측하는 확률 모델을 **언어 모델(Language Model)이라고 부른다.  \n",
        "이런 언어 모델을 어떻게 학습시킬 수 있을까? 간단하다. 어떤 텍스트도 언어 모델의 학습 데이터가 될 수 있다. n-1번째까지의 단어 시퀀스가 x_train이 되고, n번째 단어가 y_train이 되는 데이터셋은 무궁무진하게 만들 수 있기 때문이다.  \n",
        "\n",
        "이렇게 학습된 언어 모델을 학습 모드가 아닌 테스트 모드로 가동하면 어떤 일이 벌어질까? 이 모델은 일정한 단어 시퀀스가 주어지면 다음 단어, 그 다음 단어를 계속해서 예측해 낼 것이다. 이게 바로 텍스트 생성이고 작문이 된다. **잘 학습된 언어 모델은 훌륭한 문장 생성기**로 동작하게 된다."
      ],
      "metadata": {
        "id": "OGKVH5oxAI5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 파일을 읽기모드로 열고\n",
        "# 라인 단위로 끊어서 list 형태로 읽어온다.\n",
        "file_path = '/content/data/shakespeare.txt'\n",
        "with open(file_path, \"r\") as f:\n",
        "  raw_corpus = f.read().splitlines()\n",
        "\n",
        "# 앞에서부터 10라인만 화면에 출력\n",
        "print(raw_corpus[:9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq9eXbDEC4d3",
        "outputId": "1a177bad-c9dd-434c-87d9-0fef20578355"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "완벽한 연극 대본이다. 하지만 문장(대사)만을 원하므로 화자 이름이나 공백뿐인 정보는 필요가 없다. 지금 만들 모델은 연극 대사를 만들어 내는 모델이다.  \n",
        "1차 필터링할 필요가 있으므로 데이터의 형태를 자세히 살피며 필터를 구상해보자.  \n",
        "우리가 원치 않는 문장은 화자가 표기된 문장, 공백인 문장이다. 화자가 표기된 문장의 끝은 `:`로 끝나는데, 일반적으로 대사가 `:`로 끝나는 일은 없으니, `:`를 기준으로 문장을 제외해도 괜찮을 것이다. 그리고 공백인 문자는 길이를 검사해 길이가 0이라면 제외시킨다."
      ],
      "metadata": {
        "id": "if-UNMMXDkWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, sentence in enumerate(raw_corpus):\n",
        "  if len(sentence) == 0: continue # 길이가 0인 문장은 건너뛴다.\n",
        "  if sentence[-1] == \":\": continue  # 문장의 끝이 :인 문장은 건너뛴다.\n",
        "\n",
        "  if idx > 9: break # 문장 10개만 확인\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zj7u2YJbEH5R",
        "outputId": "dc0a37bd-8b7d-4183-ca3e-bbc5477668d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before we proceed any further, hear me speak.\n",
            "Speak, speak.\n",
            "You are all resolved rather to die than to famish?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리가 원하는 문장만 성공적으로 출력된다.  \n",
        "\n",
        "텍스트 분류 모델과 마찬가지로 텍스트 생성 모델도 단어 사전을 만들게 된다. 그렇다면 문장을 일정한 기준으로 쪼개야하는데, 그 과정을 **토큰화(Tokenize)**라고 한다.  \n",
        "\n",
        "가장 심플한 방법은 띄어쓰기를 기준으로 나누는 방법인데, 약간의 문제가 있을 수 있다. \n",
        "\n",
        "1. Hi, my name is John. (\"Hi,\", \"my\", \"john.\"으로 분리됨) - 문장부호\n",
        "2. First, open the first chapter. (First와 first를 다른 단어로 인식) - 대소문자\n",
        "3. He is a ten-year-old boy. (ten-year-old를 한 단어로 인식) - 특수문자  \n",
        "\n",
        "`1`을 막기 위해 **문장 부호 양쪽에 공백을 추가**하고, `2`를 막기 위해 **모든 문자를 소문자료 변환**한다. `3`을 막기 위해 **특수문자는 모두 제거**한다.  \n",
        "\n",
        "이러한 전처리를 위해 정규표현식(regex)을 이용한 필터링이 유용하게 사용된다."
      ],
      "metadata": {
        "id": "or_D1_2CEi5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력된 문장을\n",
        "#     1. 소문자로 바꾸고, 양쪽 공백을 지운다.\n",
        "#     2. 특수문자 양쪽에 공백을 넣고\n",
        "#     3. 여러 개의 공백은 하나의 공백으로 바꿉니다\n",
        "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾼다.\n",
        "#     5. 다시 양쪽 공백을 지운다.\n",
        "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() # 1\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
        "    sentence = sentence.strip() # 5\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
        "    return sentence\n",
        "\n",
        "# 이 문장이 어떻게 필터링되는지 확인\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPjVs6JYIkkB",
        "outputId": "bd12ce37-8056-469a-9c36-80803b0b5d95"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> this is sample sentence . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "자연어처리 분야에서 모델의 입력이 되는 문장을 **소스 문장(Source Sentence)**, 정답 역할을 하게 될 모델의 출력 문장을 **타겟 문장(Target Sentence)**라고 관례적으로 부른다. 각각 X_train, y_train에 해당한다고 볼 수 있다.  \n",
        "\n",
        "위에서 만든 정제 함수를 통해 만든 데이터셋에서 토큰화를 진행한 후, 끝 단어 <end>`를 없애면 소스 문장, 첫 단어 `<start>`를 없애면 타겟 문장이 된다. "
      ],
      "metadata": {
        "id": "JVxibBRJJVt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 여기에 정제된 문장을 모은다.\n",
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "  # 원치 않는 문장은 건너뛴다.\n",
        "  if len(sentence) == 0: continue\n",
        "  if sentence[-1] == \":\": continue\n",
        "\n",
        "  # 정제를 하고 담는다.\n",
        "  preprocessed_sentence = preprocess_sentence(sentence)\n",
        "  corpus.append(preprocessed_sentence)\n",
        "\n",
        "# 정제된 결과 10개 확인\n",
        "corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th6fv5IMKBqu",
        "outputId": "b9bfbee4-7ef4-4218-a444-44c012d5b3fa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> before we proceed any further , hear me speak . <end>',\n",
              " '<start> speak , speak . <end>',\n",
              " '<start> you are all resolved rather to die than to famish ? <end>',\n",
              " '<start> resolved . resolved . <end>',\n",
              " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
              " '<start> we know t , we know t . <end>',\n",
              " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
              " '<start> is t a verdict ? <end>',\n",
              " '<start> no more talking on t let it be done away , away ! <end>',\n",
              " '<start> one word , good citizens . <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텐서플로우는 자연어 처리를 위한 여러 가지 모듈을 제공하는데, `tf.keras.preprocessing.text.Tokenizer` 패키지는 정제된 데이터를 토큰화하고, 단어 사전(vocabulary / dictionary)을 만들어주며, 데이터를 숫자로 변환까지 한 번에 해준다. 이 과정을 **벡터화(vectorize)**라 하며, 숫자로 변환된 데이터를 **텐서(tensor)**라고 칭한다. 텐서플로우로 만든 모델의 입출력 데이터는 실제로는 모두 이런 텐서로 변환되어 처리되는 것이다.  \n",
        ">텐서(tensor)는 굉장히 어려운 물리학 및 수학 개념이다. 그 내용을 모두 이해할 필요는 없으나 아래의 웹페이지가 설명하는 간단한 개념 정도는 알고 있으면 좋다.  \n",
        " - [Tensor란 무엇인가?](https://rekt77.tistory.com/102)"
      ],
      "metadata": {
        "id": "S2-ezaa5Kwvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용한다.\n",
        "# 더 잘 알기 위해 아래 문서들을 참고\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
        "def tokenize(corpus):\n",
        "    # 7000단어를 기억할 수 있는 tokenizer를 만든다.\n",
        "    # 이미 문장을 정제했으니 filters가 필요 없다. \n",
        "    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꾼다.\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=7000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    # corpus를 이용해 tokenizer 내부의 단어장을 완성한다.\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환한다.\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춘다.\n",
        "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춘다.\n",
        "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36x10yEbM8rA",
        "outputId": "6218e293-51a3-4dca-c38d-1f3b304c2d73"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2  143   40 ...    0    0    0]\n",
            " [   2  110    4 ...    0    0    0]\n",
            " [   2   11   50 ...    0    0    0]\n",
            " ...\n",
            " [   2  149 4553 ...    0    0    0]\n",
            " [   2   34   71 ...    0    0    0]\n",
            " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f096aa7b550>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "생성된 텐서 데이터를 3번째 행, 10번째 열까지만 출력해보자.\n",
        "\n"
      ],
      "metadata": {
        "id": "LFesntm1N7XT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor[:3, :10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kShXc5ZWN-xg",
        "outputId": "11231012-9431-4d6d-8bab-26310076d60d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2  143   40  933  140  591    4  124   24  110]\n",
            " [   2  110    4  110    5    3    0    0    0    0]\n",
            " [   2   11   50   43 1201  316    9  201   74    9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텐서 데이터는 모두 정수로 이루어져 있다. 이 숫자는 다름 아니라, tokenizer에 구축된 단어 사전의 인덱스다. 단어 사전이 어떻게 구축되었는지 확인해보자."
      ],
      "metadata": {
        "id": "kyjdbkIbOPyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in tokenizer.index_word:\n",
        "  print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "  if idx >=10: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7IGtVUvOWtC",
        "outputId": "3f645047-9319-4404-e420-ce3b63d7f48e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : ,\n",
            "5 : .\n",
            "6 : the\n",
            "7 : and\n",
            "8 : i\n",
            "9 : to\n",
            "10 : of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2번 인덱스가 바로 `<start>`였다. 모든 행이 왜 2로 시작하는지 이해할 수 있다.  \n",
        "이제 생성된 텐서를 소스와 타겟으로 분리해 모델이 학습할 수 있게 하자. 이 과정도 텐서플로우가 제공하는 모듈을 사용할 것이니, 어떻게 사용하는지만 눈여겨 보자.  \n",
        "\n",
        "텐서 출력부에서 행 뒤에 0이 많이 나온 부분은 정해진 입력 시퀀스 길이보다 문장이 짧을 경우 0으로 패딩(padding)을 채워넣은 것이다. 사전에는 없지만 0은 바로 패딩 문자 `<pad>`가 될 것이다."
      ],
      "metadata": {
        "id": "rU53bAgfeVzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성한다.\n",
        "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높다.\n",
        "src_input = tensor[:, :-1]\n",
        "# tensor에서 <start>를 ㅈ랄라내서 타겟 문장을 생성한다.\n",
        "tgt_input = tensor[:, 1:]\n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaoBP8f-fUJn",
        "outputId": "eae54ab9-b144-431e-f41b-0d3a90262256"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
            "   0   0]\n",
            "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
            "   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "corpus 내의 첫 번째 문장에 대해 생성된 소스와 타겟 문장을 확인해보았다. 예상대로 소스는 2(`<start>`)로 시작해서 3(`<end>`)로 끝난 후 0(`<pad>`)로 채워져 있다. 하지만 타겟은 2로 시작하지 않고 소스를 왼쪽으로 한 칸 시프트 형태를 갖고 있다.  \n",
        "\n",
        "마지막으로 데이터셋 객체를 생성한다. `tf.data.Dataset` 객체를 생성하는 방법을 흔히 사용한다. `tf.data.Dataset` 객체는 텐서플로우에서 사용할 경우 데이터 입력 파이프라인을 통한 속도 개선 및 각종 편의 기능을 제공한다.  \n",
        "이미 데이터셋을 텐서 형태로 생성했기 때문에, `tf.data.Dataset.from_tensor_slices()` 메서드를 이용해 `tf.data.Dataset` 객체를 생성한다. "
      ],
      "metadata": {
        "id": "Bs54ot7bfp-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        "# tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함해 7001개\n",
        "VOCAB_SIZE = tokenizer.num_words + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS5xWCjLgd8c",
        "outputId": "67c5dead-45a2-4462-e698-3639f3085366"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "    self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "    self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "    self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, x):\n",
        "    out = self.embedding(x)\n",
        "    out = self.rnn_1(out)\n",
        "    out = self.rnn_2(out)\n",
        "    out = self.linear(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "embedding_size = 256\n",
        "hidden_size = 1024\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)"
      ],
      "metadata": {
        "id": "WnVgtZ26knyg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `embedding_size` : 워드 벡터의 차원 수, 즉 단어가 추상적으로 표현되는 크기. \n",
        "- `hidden_size` : LSTM 레이어의 hidden state의 차원 수"
      ],
      "metadata": {
        "id": "B-A9QjdUlhHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋에서 데이터 한 배치만 로드\n",
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "# 한 배치만 로드한 데이터를 모델에 넣음\n",
        "model(src_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AY17wy5vlzhV",
        "outputId": "1de3fe92-c05d-483b-b746-03976865fc5c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
              "array([[[ 3.23315122e-04,  5.84778143e-04, -3.33387434e-04, ...,\n",
              "          3.11978965e-06,  8.72639357e-05,  1.73452878e-04],\n",
              "        [ 3.75213276e-04,  8.24246963e-04, -2.05786942e-04, ...,\n",
              "          3.16691112e-05,  1.23073536e-04,  4.08579537e-04],\n",
              "        [ 4.12694673e-04,  1.09086931e-03,  7.20844400e-05, ...,\n",
              "          8.27757540e-05, -5.55248407e-05,  3.82970000e-04],\n",
              "        ...,\n",
              "        [-9.54871590e-04,  1.37437345e-03,  4.91933338e-03, ...,\n",
              "          1.49647368e-03,  5.09794382e-03, -1.11690475e-04],\n",
              "        [-8.77874554e-04,  1.44473708e-03,  5.46943303e-03, ...,\n",
              "          1.60472211e-03,  5.59673365e-03, -1.61095479e-04],\n",
              "        [-7.87832600e-04,  1.51223596e-03,  5.96571201e-03, ...,\n",
              "          1.70027965e-03,  6.01692731e-03, -1.93378888e-04]],\n",
              "\n",
              "       [[ 3.23315122e-04,  5.84778143e-04, -3.33387434e-04, ...,\n",
              "          3.11978965e-06,  8.72639357e-05,  1.73452878e-04],\n",
              "        [ 2.20956863e-05,  1.10835792e-03, -6.65981031e-04, ...,\n",
              "          2.04646094e-05,  8.53261663e-05,  3.40204511e-04],\n",
              "        [ 2.48126198e-05,  1.74557662e-03, -9.21928440e-04, ...,\n",
              "         -1.23167178e-04,  1.35032664e-04, -4.74162243e-06],\n",
              "        ...,\n",
              "        [-5.85763599e-04,  2.10264744e-03,  3.28046875e-03, ...,\n",
              "          2.00410490e-03,  2.94648041e-03, -3.82614729e-04],\n",
              "        [-6.40197017e-04,  2.00629514e-03,  4.01165197e-03, ...,\n",
              "          1.99112180e-03,  3.68891330e-03, -3.35311430e-04],\n",
              "        [-6.63901970e-04,  1.93437410e-03,  4.67811711e-03, ...,\n",
              "          1.98232825e-03,  4.35428461e-03, -2.95942649e-04]],\n",
              "\n",
              "       [[ 3.23315122e-04,  5.84778143e-04, -3.33387434e-04, ...,\n",
              "          3.11978965e-06,  8.72639357e-05,  1.73452878e-04],\n",
              "        [ 8.26753167e-05,  1.01436488e-03, -1.06160261e-03, ...,\n",
              "         -5.45596085e-05, -5.14162384e-05,  8.95765770e-05],\n",
              "        [-6.15175031e-06,  1.30186370e-03, -1.47231901e-03, ...,\n",
              "         -2.04343305e-04, -4.74954795e-05, -5.90440213e-05],\n",
              "        ...,\n",
              "        [-1.07328128e-03,  1.70406082e-03,  2.45556212e-03, ...,\n",
              "          5.76335122e-04,  1.77174900e-03, -6.47321358e-05],\n",
              "        [-1.10713742e-03,  1.70437770e-03,  3.16349068e-03, ...,\n",
              "          8.05300311e-04,  2.55339453e-03, -1.07319851e-04],\n",
              "        [-1.11445470e-03,  1.71971123e-03,  3.82944802e-03, ...,\n",
              "          1.01038080e-03,  3.28667229e-03, -1.41454599e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 3.23315122e-04,  5.84778143e-04, -3.33387434e-04, ...,\n",
              "          3.11978965e-06,  8.72639357e-05,  1.73452878e-04],\n",
              "        [ 6.50780683e-04,  8.71209661e-04, -1.25232400e-04, ...,\n",
              "          1.68306797e-04,  2.35485117e-04,  2.05924254e-04],\n",
              "        [ 7.27728650e-04,  1.03073334e-03, -4.17067031e-06, ...,\n",
              "          3.41210136e-04,  3.01894645e-04, -3.78665136e-05],\n",
              "        ...,\n",
              "        [ 3.40806728e-04,  1.59644056e-03,  4.59641172e-03, ...,\n",
              "          1.82078988e-03,  4.15087864e-03,  2.71590077e-04],\n",
              "        [ 2.42303111e-04,  1.65541912e-03,  5.17404033e-03, ...,\n",
              "          1.91180536e-03,  4.68277326e-03,  2.24784104e-04],\n",
              "        [ 1.72989283e-04,  1.70880125e-03,  5.68746449e-03, ...,\n",
              "          1.98991480e-03,  5.14917169e-03,  1.80932941e-04]],\n",
              "\n",
              "       [[ 3.23315122e-04,  5.84778143e-04, -3.33387434e-04, ...,\n",
              "          3.11978965e-06,  8.72639357e-05,  1.73452878e-04],\n",
              "        [ 4.82820586e-04,  1.03339017e-03, -3.14395991e-04, ...,\n",
              "         -4.16214316e-04,  2.84887301e-05,  5.05659147e-04],\n",
              "        [ 6.31125702e-04,  1.05833623e-03, -2.30491205e-04, ...,\n",
              "         -4.20066208e-04, -7.41463373e-05,  7.25660182e-04],\n",
              "        ...,\n",
              "        [-4.13046218e-04,  1.85739016e-03,  6.49259752e-03, ...,\n",
              "          1.70711859e-03,  6.24768622e-03, -6.82075042e-05],\n",
              "        [-3.88474786e-04,  1.87540741e-03,  6.85206754e-03, ...,\n",
              "          1.78658613e-03,  6.52748626e-03, -6.14880191e-05],\n",
              "        [-3.49200593e-04,  1.89132686e-03,  7.16720195e-03, ...,\n",
              "          1.85751740e-03,  6.75755646e-03, -5.57208077e-05]],\n",
              "\n",
              "       [[ 3.23315122e-04,  5.84778143e-04, -3.33387434e-04, ...,\n",
              "          3.11978965e-06,  8.72639357e-05,  1.73452878e-04],\n",
              "        [ 3.55661818e-04,  8.15105159e-04, -7.12695706e-04, ...,\n",
              "          4.02753154e-04,  6.10868272e-04,  5.82028413e-04],\n",
              "        [-4.69133920e-05,  8.26271076e-04, -7.54305045e-04, ...,\n",
              "          3.97858967e-04,  1.02786918e-03,  5.88468218e-04],\n",
              "        ...,\n",
              "        [-9.78992670e-04,  8.95678299e-04,  3.93050071e-03, ...,\n",
              "          2.22775037e-03,  4.07811441e-03,  1.20111246e-04],\n",
              "        [-9.87721956e-04,  1.02946593e-03,  4.57454333e-03, ...,\n",
              "          2.21460895e-03,  4.66241781e-03,  1.03332641e-04],\n",
              "        [-9.62932885e-04,  1.15809008e-03,  5.15935663e-03, ...,\n",
              "          2.19935179e-03,  5.17764641e-03,  8.85343543e-05]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델의 최종 출력 텐서 shape은 `shape=(256, 20, 7001)`이다. 7001은 Dense ㄹ이어의 출력 차원 수, 256은 배치 사이즈, 20은 시퀀스 길이다. "
      ],
      "metadata": {
        "id": "hL1B33HRmUkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL9QYyQbmqLA",
        "outputId": "dbc4345d-531e-4ac9-a48d-696b53a04612"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  1792256   \n",
            "                                                                 \n",
            " lstm (LSTM)                 multiple                  5246976   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               multiple                  8392704   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  7176025   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,607,961\n",
            "Trainable params: 22,607,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(dataset, epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvrd2ObSmz_2",
        "outputId": "35ba0a94-2882-4c02-f861-2a287b5b9b64"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "93/93 [==============================] - 37s 365ms/step - loss: 3.5592\n",
            "Epoch 2/30\n",
            "93/93 [==============================] - 34s 364ms/step - loss: 2.8219\n",
            "Epoch 3/30\n",
            "93/93 [==============================] - 34s 364ms/step - loss: 2.7440\n",
            "Epoch 4/30\n",
            "93/93 [==============================] - 34s 365ms/step - loss: 2.6550\n",
            "Epoch 5/30\n",
            "93/93 [==============================] - 34s 364ms/step - loss: 2.5797\n",
            "Epoch 6/30\n",
            "93/93 [==============================] - 34s 364ms/step - loss: 2.5260\n",
            "Epoch 7/30\n",
            "93/93 [==============================] - 34s 362ms/step - loss: 2.4824\n",
            "Epoch 8/30\n",
            "93/93 [==============================] - 34s 361ms/step - loss: 2.4335\n",
            "Epoch 9/30\n",
            "93/93 [==============================] - 34s 361ms/step - loss: 2.3895\n",
            "Epoch 10/30\n",
            "93/93 [==============================] - 34s 363ms/step - loss: 2.3511\n",
            "Epoch 11/30\n",
            "93/93 [==============================] - 34s 362ms/step - loss: 2.3137\n",
            "Epoch 12/30\n",
            "93/93 [==============================] - 34s 360ms/step - loss: 2.2777\n",
            "Epoch 13/30\n",
            "93/93 [==============================] - 34s 361ms/step - loss: 2.2415\n",
            "Epoch 14/30\n",
            "93/93 [==============================] - 34s 363ms/step - loss: 2.2066\n",
            "Epoch 15/30\n",
            "93/93 [==============================] - 34s 361ms/step - loss: 2.1718\n",
            "Epoch 16/30\n",
            "93/93 [==============================] - 34s 364ms/step - loss: 2.1379\n",
            "Epoch 17/30\n",
            "93/93 [==============================] - 34s 362ms/step - loss: 2.1046\n",
            "Epoch 18/30\n",
            "93/93 [==============================] - 34s 361ms/step - loss: 2.0717\n",
            "Epoch 19/30\n",
            "93/93 [==============================] - 34s 361ms/step - loss: 2.0369\n",
            "Epoch 20/30\n",
            "93/93 [==============================] - 34s 361ms/step - loss: 2.0038\n",
            "Epoch 21/30\n",
            "93/93 [==============================] - 34s 361ms/step - loss: 1.9714\n",
            "Epoch 22/30\n",
            "93/93 [==============================] - 34s 361ms/step - loss: 1.9364\n",
            "Epoch 23/30\n",
            "93/93 [==============================] - 34s 362ms/step - loss: 1.9052\n",
            "Epoch 24/30\n",
            "93/93 [==============================] - 34s 360ms/step - loss: 1.8703\n",
            "Epoch 25/30\n",
            "93/93 [==============================] - 34s 360ms/step - loss: 1.8364\n",
            "Epoch 26/30\n",
            "93/93 [==============================] - 33s 359ms/step - loss: 1.8038\n",
            "Epoch 27/30\n",
            "93/93 [==============================] - 33s 359ms/step - loss: 1.7698\n",
            "Epoch 28/30\n",
            "93/93 [==============================] - 34s 361ms/step - loss: 1.7382\n",
            "Epoch 29/30\n",
            "93/93 [==============================] - 34s 360ms/step - loss: 1.7057\n",
            "Epoch 30/30\n",
            "93/93 [==============================] - 33s 360ms/step - loss: 1.6750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f08eea25e10>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "  # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환\n",
        "  test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "  test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "  end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "  while True:\n",
        "    predict = model(test_tensor)\n",
        "    predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n",
        "    test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "    \n",
        "    if predict_word.numpy()[0] == end_token: break\n",
        "    if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "  generated = \"\"\n",
        "  for word_index in test_tensor[0].numpy():\n",
        "    generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "  return generated\n"
      ],
      "metadata": {
        "id": "YBxkl8q7sjEP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LJbUvRjsvWij",
        "outputId": "34769935-a1d2-4924-b1ad-c7bfca2774b2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> i ll tell you , sir , i ll not be slack to do . <end> '"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}