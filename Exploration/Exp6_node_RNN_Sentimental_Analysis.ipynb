{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56857fe",
   "metadata": {},
   "source": [
    "# Exp6. Sentimental Analysis - RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55221f33",
   "metadata": {},
   "source": [
    "### 텍스트를 숫자로 표현하기\n",
    "텍스트는 그 자체로는 기호일 뿐이고, 텍스트가 내포하는 의미를 기호가 직접 내포하지 않는다.  \n",
    "그렇기에 우리는 단어 사전을 만들어 단어와 단어의 의미를 나타내는 벡터를 짝지어 볼 수 있다. \n",
    ">i feel hungry  \n",
    "i eat lunch  \n",
    "now i feel happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01f46584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'feel', 'hungry']\n"
     ]
    }
   ],
   "source": [
    "sentences = ['i feel hungry', 'i eat lunch', 'now i feel happy']\n",
    "\n",
    "# split() 메서드로 단어 단위로 쪼개 보자.\n",
    "word_list = 'i feel hungry'.split()\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50eaca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<BOS>', 2: '<UNK>', 3: 'i', 4: 'feel', 5: 'hungry', 6: 'eat', 7: 'lunch', 8: 'now', 9: 'happy'}\n"
     ]
    }
   ],
   "source": [
    "# 빈 딕셔너리 생성\n",
    "index_to_word = {}\n",
    "\n",
    "# <BOS>, <PAD>, <UNK>는 관례적으로 딕셔너리 맨 앞에 넣는다.\n",
    "index_to_word[0] = '<PAD>'  # 패딩용 단어\n",
    "index_to_word[1] = '<BOS>'  # 문장의 시작지점\n",
    "index_to_word[2] = '<UNK>'  # 사전에 없는(Unknown) 단어\n",
    "index_to_word[3] = 'i'\n",
    "index_to_word[3] = 'i'\n",
    "index_to_word[4] = 'feel'\n",
    "index_to_word[5] = 'hungry'\n",
    "index_to_word[6] = 'eat'\n",
    "index_to_word[7] = 'lunch'\n",
    "index_to_word[8] = 'now'\n",
    "index_to_word[9] = 'happy'\n",
    "\n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e5fec40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<BOS>': 1, '<UNK>': 2, 'i': 3, 'feel': 4, 'hungry': 5, 'eat': 6, 'lunch': 7, 'now': 8, 'happy': 9}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {word:index for index, word in index_to_word.items()}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11fcef3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# 이제 이 딕셔너리는 단어를 주면 그 단어의 인덱스를 반환한다.\n",
    "print(word_to_index['feel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf79eaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 주면, 단어 인덱스 리스트로 변환\n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']] + [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "print(get_encoded_sentence('i eat lunch', word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee897635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]]\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 문장 리스트를 한꺼번에 숫자 텐서로 encode\n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f0f102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i feel hungry\n"
     ]
    }
   ],
   "source": [
    "# 숫자 벡터로 encode된 문장을 원래대로 decode\n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])\n",
    "\n",
    "print(get_decoded_sentence([1, 3, 4, 5], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e4167db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i feel hungry', 'i eat lunch', 'now i feel happy']\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 decode\n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "print(get_decoded_sentences(encoded_sentences, index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb1feb",
   "metadata": {},
   "source": [
    "### Embedding 레이어\n",
    "위에서 만든 벡터는 텍스트에 담긴 언어의 의미와 대응되는 벡터가 아닌, 임의로 부여된 단어의 순서에 불과하다.  \n",
    "단어와 그 단어의 의미를 나타내는 벡터를 짝지어 훈련 가능한 파라미터로 놓고 이를 딥러닝을 통해 학습해 최적화해야 한다.  \n",
    "Tensorfow, Pytorch 등의 딥러닝 프레임워크들은 이러한 의미 벡터 파라미터를 구현한 **Embedding 레이어**를 제공한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb24ce65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.04614459 -0.01872499 -0.0093254  -0.03130987]\n",
      "  [-0.00157475  0.01702125  0.03673667  0.020668  ]\n",
      "  [-0.01727836 -0.0365799  -0.00639813  0.00749482]\n",
      "  [-0.03550913 -0.02033593 -0.00361381 -0.012744  ]\n",
      "  [ 0.00682653  0.04289414  0.04266309  0.00174015]]\n",
      "\n",
      " [[-0.04614459 -0.01872499 -0.0093254  -0.03130987]\n",
      "  [-0.00157475  0.01702125  0.03673667  0.020668  ]\n",
      "  [-0.04226501  0.02292699 -0.0214425  -0.01398691]\n",
      "  [-0.04074789 -0.02181323  0.01800436 -0.00130145]\n",
      "  [ 0.00682653  0.04289414  0.04266309  0.00174015]]\n",
      "\n",
      " [[-0.04614459 -0.01872499 -0.0093254  -0.03130987]\n",
      "  [-0.01611158 -0.04577921  0.0384709  -0.04793768]\n",
      "  [-0.00157475  0.01702125  0.03673667  0.020668  ]\n",
      "  [-0.01727836 -0.0365799  -0.00639813  0.00749482]\n",
      "  [-0.01464113  0.03252186  0.02412741  0.00850205]]], shape=(3, 5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "word_vector_dim = 4  # 4차원의 워드 벡터를 가정\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype='object')\n",
    "# 주의! Embedding 레이어의 input이 되는 문장 벡터는 그 길이가 일정해야 한다!\n",
    "# 아래 코드를 통해 문장 벡터 뒤에 <pad>를 추가해 길이를 일정하게 맞춘다!\n",
    "raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs, value=word_to_index['<PAD>'], padding='post', maxlen=5)\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f67b39",
   "metadata": {},
   "source": [
    "output의 `shape=(3, 5, 4)`는 순서대로 입력문장의 개수, 입력문장의 최대 길이, 워드 벡터의 차원 수를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16a63f4",
   "metadata": {},
   "source": [
    "### 시퀀스 데이터를 다루는 RNN\n",
    "텍스트 데이터를 다루는 데 주로 사용되는 딥러닝 모델은 `RNN(Recurrent Neural Network)`이다. 이는 시퀀스 형태의 데이터를 처리하기에 죄적의 모델로 알려져있다.  \n",
    "RNN은 시간의 흐름에 따라 새롭게 들어오는 입력에 따라 변하는 현재 상태를 묘사하는 state machine으로 설계되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c8f043e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 416       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 537\n",
      "Trainable params: 537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# RNN 모델을 사용해 이전 스텝의 텍스트 데이터를 처리하는 예제\n",
    "vocab_size = 10\n",
    "word_vector_dim = 4\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(8)) # 널리 쓰이는 RNN인 LSTM 레이어 사용, LSTM state 벡터의 차원 수는 8로 설정\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824fe8c8",
   "metadata": {},
   "source": [
    "### 1-D CNN(Convolution Neural Network) vs RNN\n",
    "텍스트를 처리하기 위해 RNN이 아니라 `1-D CNN`을 사용할 수도 있다.  \n",
    "`1-D CNN`은 문장 전체를 한꺼번에 한 방향으로 길이 7짜리 필터로 스캔하면서 7단어 이내에서 발견되는 특징을 추출해 그것으로 문장을 분류한다.  \n",
    "이 방식도 텍스트를 처리하는데 RNN 못지않은 효율을 보여주는데, RNN 계열보다 병렬처리가 효율적이기 대문에 학습 속도도 훨씬 빠르다는 장점이 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "333a2152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          464       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,457\n",
      "Trainable params: 2,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "word_vector_dim = 4\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None, )))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd324fb",
   "metadata": {},
   "source": [
    "아주 간단히는 `GlobalMaxPooling1D()` 레이어 하나만 사용하는 방법도 생각해 볼 수 있다.  \n",
    "이 방식은 전체 문장 중 가장 중요한 단어만 피처로 추추래 그것으로 문장의 긍정 / 부정을 평가하는 방식인데, 의외로 성능이 잘 나올 수도 있다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "576553c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 89\n",
      "Trainable params: 89\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "word_vector_dim = 4\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507cec01",
   "metadata": {},
   "source": [
    "1-D CNN과 RNN 레이어를 섞는다던가, FFN(FeedFoward Network) 레이어만으로 구성하거나, 최근 주목받는 Trnsformer 레이어를 사용하는 등 다양한 시도가 가능하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3225e21",
   "metadata": {},
   "source": [
    "## IMDB 영화 리뷰 감성분석\n",
    "### (1) IMDB 데이터셋 분석\n",
    "IMDB Large Movie Dataset은 50000개의 영어로 작성된 영화 리뷰 텍스트로, 긍정은 1, 부정은 0의 라벨이 달려있다.  \n",
    "절반인 25000개가 훈련용 데이터, 나머지를 테스트용 데이터로 사용하도록 지정되어 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7958864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 개수: 25000, 테스트 개수: 25000\n"
     ]
    }
   ],
   "source": [
    "imdb = tf.keras.datasets.imdb\n",
    "\n",
    "# 단어사전에 등재할 단어의 개수를 10000으로 지정하면, 그 개수만큼의 word_to_index 딕셔너리까지 생성된 상태로 데이터셋 생성\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(x_train), len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cd2a3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "라벨:  1\n",
      "1번째 리뷰 문장 길이:  218\n",
      "2번째 리뷰 문장 길이:  189\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print('라벨: ', y_train[0])\n",
    "print('1번째 리뷰 문장 길이: ', len(x_train[0]))\n",
    "print('2번째 리뷰 문장 길이: ', len(x_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0495c1ea",
   "metadata": {},
   "source": [
    "텍스트 데이터가 아니라 이미 숫자로 encode된 텍스트 데이터를 다운로드했음을 알 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83f478ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "print(index_to_word[1])\n",
    "print(word_to_index['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d701c1b3",
   "metadata": {},
   "source": [
    "`word_to_index`, `index_to_word`는 다음과 같이 보정되어야 한다. (Tensorflow 튜토리얼의 가이드를 참고했다)  \n",
    "`word_to_index`는 IMDB 텍스트 데이터셋의 단어 출현 빈도 기준 내림차순 정렬되어 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e55c242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>\n",
      "4\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "# 실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있다.\n",
    "word_to_index = {k:(v+3) for k, v in word_to_index.items()}\n",
    "\n",
    "# 처음 몇 개의 인덱스는 사전에 정의되어 있다.\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2\n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "print(index_to_word[1])\n",
    "print(word_to_index['the'])\n",
    "print(index_to_word[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "492eb88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "라벨:  1\n"
     ]
    }
   ],
   "source": [
    "print(get_decoded_sentence(x_train[0], index_to_word))\n",
    "print('라벨: ', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a1dce4",
   "metadata": {},
   "source": [
    "`pad_sequences`를 통해 데이터셋 상의 문장의 길이를 통일하는 것을 잊어서는 안 된다.  \n",
    "문장의 최대 길이 `maxlen`의 값 설정도 전체 모델 성능에 영향을 미친다. 적절한 값을 찾기 위해서는 전체 데이터셋의 분포를 확인해보는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7a7b61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 길이 평균:  234.75892\n",
      "문장 길이 최대:  2494\n",
      "문장 길이 표준편차:  172.91149458735703\n",
      "pad_sequences maxlen:  580\n",
      "전체 문장의 0.94536%가 maxlen 설정값 이내에 포함된다.\n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "# 문장 길이의 평균, 최대값, 표준편차 계산\n",
    "print('문장 길이 평균: ', np.mean(num_tokens))\n",
    "print('문장 길이 최대: ', np.max(num_tokens))\n",
    "print('문장 길이 표준편차: ', np.std(num_tokens))\n",
    "\n",
    "# 예시로, 최대 길이를 *(평균 + 2 * 표준편차)로 설정할 경우\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen: ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함된다.'.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d730ef0",
   "metadata": {},
   "source": [
    "또 한 가지 유의해야 할 점은 padding 방식을 post와 pre 중 어느 쪽으로 정하느냐에 따라 RNN을 이용한 딥러닝 적용 시 성능 차이가 발생한다.  \n",
    "두 가지 방식을 한 번씩 다 적용해서 그 결과를 비교해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6aabcb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 580)\n"
     ]
    }
   ],
   "source": [
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, value=word_to_index[\"<PAD>\"],\n",
    "                                                      padding='post',\n",
    "                                                      maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bda2d7",
   "metadata": {},
   "source": [
    ">RNN은 입력데이터가 순차적으로 처리되어, 가장 마지막 입력이 최종 state 값에 가장 영향을 많이 미치게 된다.  \n",
    "따라서 마지막 입력이 무의미한 padding으로 채워지는 것은 비효율적이다. 따라서 'pre'가 훨씬 유리하며, 10% 이상의 테스트 성능 차이를 보이게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152ef74",
   "metadata": {},
   "source": [
    "### (2) 딥러닝 모델 설계와 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de19549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 163,761\n",
      "Trainable params: 163,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 설계\n",
    "vocab_size = 10000\n",
    "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼 파라미터)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None, )))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3497dd7",
   "metadata": {},
   "source": [
    "model 훈련 전에, 훈련용 데이터셋 25000건 중 10000건을 분리해 validation set으로 사용하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ee9d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 580)\n",
      "(15000,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 분리\n",
    "x_val = x_train[:10000]\n",
    "y_val = y_train[:10000]\n",
    "\n",
    "partial_x_train = x_train[10000:]\n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "568ffcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 4s 31ms/step - loss: 0.6924 - accuracy: 0.5074 - val_loss: 0.6910 - val_accuracy: 0.5588\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.6808 - accuracy: 0.5402 - val_loss: 0.6741 - val_accuracy: 0.6983\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.6242 - accuracy: 0.6744 - val_loss: 0.5698 - val_accuracy: 0.7601\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.4907 - accuracy: 0.8399 - val_loss: 0.4646 - val_accuracy: 0.8426\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.3620 - accuracy: 0.9102 - val_loss: 0.3759 - val_accuracy: 0.8695\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.2326 - accuracy: 0.9447 - val_loss: 0.3306 - val_accuracy: 0.8701\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.1454 - accuracy: 0.9643 - val_loss: 0.3688 - val_accuracy: 0.8635\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.1003 - accuracy: 0.9780 - val_loss: 0.3647 - val_accuracy: 0.8651\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.0718 - accuracy: 0.9857 - val_loss: 0.4160 - val_accuracy: 0.8623\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.0514 - accuracy: 0.9914 - val_loss: 0.4311 - val_accuracy: 0.8624\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.0399 - accuracy: 0.9941 - val_loss: 0.4566 - val_accuracy: 0.8592\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.0334 - accuracy: 0.9950 - val_loss: 0.4937 - val_accuracy: 0.8588\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.0288 - accuracy: 0.9954 - val_loss: 0.5135 - val_accuracy: 0.8578\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.0259 - accuracy: 0.9959 - val_loss: 0.5408 - val_accuracy: 0.8582\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.0230 - accuracy: 0.9963 - val_loss: 0.5637 - val_accuracy: 0.8588\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.0211 - accuracy: 0.9966 - val_loss: 0.5754 - val_accuracy: 0.8561\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.0187 - accuracy: 0.9970 - val_loss: 0.5836 - val_accuracy: 0.8571\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.0173 - accuracy: 0.9971 - val_loss: 0.6129 - val_accuracy: 0.8570\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.0164 - accuracy: 0.9973 - val_loss: 0.6143 - val_accuracy: 0.8566\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.0154 - accuracy: 0.9973 - val_loss: 0.6184 - val_accuracy: 0.8565\n"
     ]
    }
   ],
   "source": [
    "# model 학습\n",
    "model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs=epochs,\n",
    "                   batch_size=512,\n",
    "                   validation_data=(x_val, y_val),\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c786efdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 1s - loss: 0.6947 - accuracy: 0.8382\n",
      "[0.6947020888328552, 0.83815997838974]\n"
     ]
    }
   ],
   "source": [
    "# test set으로 모델 평가\n",
    "results = model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c9c0e",
   "metadata": {},
   "source": [
    "`model.fit()` 과정 중 train / validation loss, accuracy 등이 epoch마다 history 변수에 저장되어 있다.   \n",
    "이 데이터를 시각화하면, 딥러닝 학습일 잘 진행되었는지, 오버피팅 혹은 언더피팅하지 않았는지, 성능을 개선할 수 있는 아이디어를 얻을 수 있는 자료가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19deb3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f3e8859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 학습 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# bo는 파란색 점, b는 파란 실선\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b45457",
   "metadata": {},
   "source": [
    "위 그래프를 보면 몇 epoch까지의 학습이 적절한지 최적점을 추정할 수 있다. validation loss의 그래프가 train loss와의 이격이 발생하게 되면 더 이상의 학습은 무의미해진다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a327933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()  # 그림 초기화\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf6c3a",
   "metadata": {},
   "source": [
    "마찬가지로 Training and validation accuracy를 그려봐도 유사한 인사이트를 얻을 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30b332d",
   "metadata": {},
   "source": [
    "### (3) Word2Vec 적용\n",
    "라벨링 비용이 많이 드는 감성분석의 비용을 줄이면서, 정확도를 크게 높일 수 있는 자연어처리 기법으로  \n",
    "단어의 특성을 저차원 벡터값으로 표현할 수 있는 `워드 임베딩(word embedding)` 기법이 있다.  \n",
    "앞서 사용했던 model의 첫 번재 레이어가 Embedding 레이어였는데, 이 레이어는 사전의 단어 개수 * 워드 벡터 사이즈만큼의 크기를 가진 학습 파라미터였다.  \n",
    "만약 앞의 모델이 학습이 잘 되었다면, Embedding 레이어에 학습된 워드 벡터들도 의미 공간 상에 유의미한 형태로 학습되었을 것이다. 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65251ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)  # vocab_size, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bd093d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 embedding 파라미터를 파일로 저장\n",
    "word2vec_file_path = os.getenv('HOME') + '/aiffel/sentiment_classification/data/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇 개의 벡터를 어떤 사이즈로 기재할지의 타이틀.\n",
    "\n",
    "# 단어 개수(특수문자 4개 제외)만큼의 워드 벡터를 파일에 기록\n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4, vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05ef5c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0115478 , -0.00109054,  0.09078266,  0.04226822, -0.03513872,\n",
       "       -0.04965927, -0.07862715, -0.07379095,  0.05347811, -0.00402092,\n",
       "        0.01751629, -0.02840046,  0.02481056,  0.07604838, -0.01475856,\n",
       "       -0.00540159], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gensim에서 제공하는 패키지로, 위에서 만든 임베딩 파라미터를 익어서 word vector로 활용\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['computer']\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf2aa01",
   "metadata": {},
   "source": [
    "워드 벡터가 의미 벡터 공간 상에 유의미하게 학습되었는지 확인하기 위해, 단어를 하나 주고 가장 유사한 단어와 그 유사도를 확인해 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "651b648d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('larger', 0.8839967846870422),\n",
       " ('believable', 0.8710741400718689),\n",
       " ('fiction', 0.861591637134552),\n",
       " ('crooks', 0.8607564568519592),\n",
       " ('athletic', 0.8585770726203918),\n",
       " ('inspector', 0.8447098135948181),\n",
       " ('discovers', 0.8367086052894592),\n",
       " ('ace', 0.8348056674003601),\n",
       " ('delirious', 0.8339557647705078),\n",
       " ('psychologically', 0.8322232365608215)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8047ac5",
   "metadata": {},
   "source": [
    "감성 분류 태스크를 잠깐 학습한 것으로는 워드 벡터가 유의미하게 학습되기 어려운 것 같다.  \n",
    "이제 구글에서 제공하는 `Word2Vec`이라는 사전학습된 워드 임베딩 모델을 활용해보자.  \n",
    ">Word2Vec은 1억 개로 구성된 Google News dataset을 바탕으로 학습되었다. 총 300만 개의 단어를 각각 300차원의 벡터로 표현했다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee78181f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_path = os.getenv('HOME') + '/aiffel/sentiment_classification/data/GoogleNews-vectors-negative300.bin.gz'\n",
    "# 300dim의 벡터로 이루어진 300만 개의 단어를 모두 메모리에 로딩하면 실습환경에 따라 메모리 에러가 날 수 있다. \n",
    "# KeyedVector.load_word2vec_format 메서드로 가장 많이 사용되는 상위 100만개만 limit으로 설정해 로드\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000)\n",
    "vector = word2vec['computer']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "688fe076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loved', 0.6907791495323181),\n",
       " ('adore', 0.6816873550415039),\n",
       " ('loves', 0.661863386631012),\n",
       " ('passion', 0.6100708842277527),\n",
       " ('hate', 0.600395679473877),\n",
       " ('loving', 0.5886635780334473),\n",
       " ('affection', 0.5664337873458862),\n",
       " ('undying_love', 0.5547304749488831),\n",
       " ('absolutely_adore', 0.5536840558052063),\n",
       " ('adores', 0.5440906882286072)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2452bc",
   "metadata": {},
   "source": [
    "앞에서 학습했던 모델의 임베딩 레이어를 Word2Vec의 것으로 교체해 다시 학습시켜보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6079b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "word_vector_dim = 300\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드 벡터를 단어 하나씩 차례로 카피\n",
    "for i in range(4, vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7712e4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 580, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 574, 16)           33616     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 114, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 108, 16)           1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 3,035,569\n",
      "Trainable params: 3,035,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000\n",
    "word_vector_dim = 300\n",
    "\n",
    "# 모델 설계\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size,\n",
    "                                   word_vector_dim,\n",
    "                                   embeddings_initializer=Constant(embedding_matrix),\n",
    "                                   input_length=maxlen,\n",
    "                                   trainable=True))  # trainable=True면 Fine-tuning\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f5d3362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 4s 82ms/step - loss: 0.6982 - accuracy: 0.5075 - val_loss: 0.6857 - val_accuracy: 0.5364\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.6647 - accuracy: 0.6067 - val_loss: 0.6517 - val_accuracy: 0.6296\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.5829 - accuracy: 0.7221 - val_loss: 0.5269 - val_accuracy: 0.7691\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.4179 - accuracy: 0.8307 - val_loss: 0.3794 - val_accuracy: 0.8409\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 0.2819 - accuracy: 0.8919 - val_loss: 0.3162 - val_accuracy: 0.8661\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 0.2006 - accuracy: 0.9293 - val_loss: 0.3165 - val_accuracy: 0.8621\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.1456 - accuracy: 0.9551 - val_loss: 0.3003 - val_accuracy: 0.8759\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.1027 - accuracy: 0.9735 - val_loss: 0.3270 - val_accuracy: 0.8672\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.0707 - accuracy: 0.9859 - val_loss: 0.3311 - val_accuracy: 0.8720\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.0473 - accuracy: 0.9938 - val_loss: 0.3481 - val_accuracy: 0.8712\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.0317 - accuracy: 0.9973 - val_loss: 0.3769 - val_accuracy: 0.8675\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.0212 - accuracy: 0.9991 - val_loss: 0.3804 - val_accuracy: 0.8745\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.0145 - accuracy: 0.9996 - val_loss: 0.3999 - val_accuracy: 0.8733\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.0099 - accuracy: 0.9998 - val_loss: 0.4158 - val_accuracy: 0.8726\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.0073 - accuracy: 0.9999 - val_loss: 0.4326 - val_accuracy: 0.8722\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.0057 - accuracy: 0.9999 - val_loss: 0.4475 - val_accuracy: 0.8720\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.0046 - accuracy: 0.9999 - val_loss: 0.4604 - val_accuracy: 0.8713\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.0037 - accuracy: 0.9999 - val_loss: 0.4724 - val_accuracy: 0.8714\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.0030 - accuracy: 0.9999 - val_loss: 0.4827 - val_accuracy: 0.8716\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4926 - val_accuracy: 0.8718\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs = 20\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2911775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 2s - loss: 0.5264 - accuracy: 0.8638\n",
      "[0.5264298915863037, 0.8637999892234802]\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
